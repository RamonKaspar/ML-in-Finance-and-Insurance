% Basic stuff
\documentclass[a4paper,10pt]{article}
\usepackage[swissgerman]{babel}

% 3 column landscape layout with fewer margins
\usepackage[landscape, left=0.5cm, top=0.75cm, right=0.75cm, bottom=1cm, footskip=15pt]{geometry}
\usepackage{flowfram}
\ffvadjustfalse
\setlength{\columnsep}{0.75cm}
\Ncolumn{3}

% Remove paragraph indentation
\setlength{\parindent}{0pt}

% For defining line spacing
\usepackage{setspace}

\usepackage{multicol}

% For inserting figures
\usepackage{wrapfig}

% For enumeration with letters
\usepackage[shortlabels]{enumitem}

% Define nice looking boxes
\usepackage[many]{tcolorbox}

% Base style for boxes
\tcbset{
  base/.style={
    boxrule=0.2mm,
    left=1.75mm,
    arc=0mm,
    fonttitle=\bfseries,
    colbacktitle=black!10!white,
    coltitle=black,
    toptitle=0.75mm,
    bottomtitle=0.25mm,
    title={#1}
  }
}
\definecolor{backcol}{RGB}{240,240,240}
\definecolor{brandblue}{rgb}{0, 0, 0}
\newtcolorbox{mainbox}[1]{
  colframe=brandblue,
  colback=backcol,
  base={#1}
}

\definecolor{backcol2}{RGB}{255,255,255}
\definecolor{brandblue2}{rgb}{0, 0, 0}
\newtcolorbox{subbox}[1]{
  colframe=black!20!white,
  colframe=brandblue2,
  colback=backcol2,
  base={#1}
}

\newtcolorbox{subbox_noTitle}[1][]{
  colframe=black!20!white,
  colframe=brandblue2,
  colback=backcol2,
  base={#1},
  title=#1
}


% Mathematical typesetting & symbols
\usepackage{amsthm, mathtools, amssymb, amsmath}
\usepackage{marvosym, wasysym}
\allowdisplaybreaks

% Algorithms
\usepackage{algorithm}
\usepackage{algpseudocode}
% Reduce space before and after algorithm
\usepackage{etoolbox}
\BeforeBeginEnvironment{algorithm}{\vspace{-1em}}
\AfterEndEnvironment{algorithm}{\vspace{-1em}}

% Tables
\usepackage{tabularx, multirow}
\usepackage{makecell}
\usepackage{booktabs}
\renewcommand*{\arraystretch}{2}

% Make enumerations more compact
\newenvironment{myitemize}
{\vspace{-0.25cm}\begin{itemize}}
{\end{itemize}}
\setlist[itemize]{leftmargin=10pt, itemsep=0.5pt}

\setlist[enumerate]{leftmargin=10pt, itemsep=0.75pt}

% To include sketches & PDFs
\usepackage{graphicx}

% For indicator function
\usepackage{dsfont}

% For hyperlinks
\usepackage{hyperref}
\hypersetup{
  colorlinks=true
}

% Metadata (fixing the inline title format)
\title{\textbf{Machine Learning in Finance and Insurance}}
\author{}
\date{}

% Reduces space above and below the title
\usepackage{titling}
\setlength{\droptitle}{-1.5cm}
% \posttitle{\par\vspace{-0.5cm}}

% Math helper stuff
\def\limn{\lim_{n\to \infty}}
\def\limxo{\lim_{x\to 0}}
\def\limxi{\lim_{x\to\infty}}
\def\limxn{\lim_{x\to-\infty}}
\def\sumk{\sum_{k=1}^\infty}
\def\sumn{\sum_{n=0}^\infty}
\def\R{\mathbb{R}}
\def\dx{\text{ d}x}
\def\P{\mathbb{P}}
\def\F{\mathcal{F}}
% \def\E{\mathcal{E}}
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\Var}{Var}

% Use small subtitles
\newcommand{\subtitle}[1]{\vspace{0.25cm}\begin{normalsize}\textbf{\textcolor{gray!150}{\uppercase{#1}}}\end{normalsize}}


\begin{document}

% Add title
\maketitle

% Reduce space
\vspace{-3cm}


\begin{center}
    \textbf{Ramon Kaspar, ETH ZÃ¼rich (Fall 2024)}
\end{center}

\begin{small}

\section{Basic Notions of Statistical Learn.}

\begin{subbox}{Expected Loss (Expected Risk)}
\[
\E[\ell(f(X), Y)] = \int_{\mathcal{X} \times \mathcal{Y}} \ell(f(x), y) \rho(dx, dy).
\]
For a measurable \textit{loss function} \(\ell : \mathcal{Y} \times \mathcal{Y} \to \R^+\)
\end{subbox}
Goal: Find \(f\) that minimizes \(\E[\ell(f(X), Y)]\).

\begin{subbox}{Empirical Loss (Empirical Risk)}
    With training data $(X_i, Y_i)_{i=1}^m$: 
    \[
        \E[\ell(f(X), Y)] \approx \frac{1}{m} \sum_{i=1}^m \ell(f(X_i), Y_i).
    \]
\end{subbox}
\(\Rightarrow\) Empirical loss minimizer depends on hypothesis class \(\mathcal{H}\) and loss function \(\ell\). For square loss and constant functions, the minimizer is \(\bar{Y} = \frac{1}{m} \sum_{i=1}^{m} Y_i\). There's a \textit{Bias-Variance Tradeoff}; the best balance depends on the unknown distribution of \(Y\).

\subtitle{Pred. Prob.:} 
Find meas. func. \(f: \mathcal{X} \rightarrow \mathcal{Y}\), s.t. \(f(X) \approx Y\).

\subtitle{Hypothesis Class:} 
A \textit{hypothesis class} is a family \(\mathcal{H}\) of measurable functions \(f : \mathcal{X} \to \mathcal{Y}\), e.g. all affine functions.

\subtitle{Model Estimation:}
Find a numerical solution \(\hat{f}_m\) to:
\begin{footnotesize}  
\[
\min_{f \in \mathcal{H}} \frac{1}{m} \sum_{i=1}^m \ell(f(X_i), Y_i)
\]
\end{footnotesize}
\begin{myitemize}
    \item \textbf{Deterministic algorithm:} \(\hat{f}_m(\cdot) = \hat{\varphi}\left( \cdot , (X_i, Y_i)_{i=1}^m \right)\)
    \item \textbf{Stochastic algorithm} (e.g., random initialization): \(\hat{f}_m(\cdot) = \hat{\varphi}\left( \cdot , (X_i, Y_i)_{i=1}^m, V \right)\), where \(V\) indep. of \((X, Y)\) \& random.
\end{myitemize}
Define the average model \(\hat{f}_m^{\text{avg}}(x) := \E[\hat{f}_m(x)]\).

\subtitle{Relationships between $X$ and $Y$:}
\vspace{0.2cm}
\begin{myitemize}
    \item \textit{Deterministic dependence:} $Y = g(X)$
    \item \textit{Homoscedastic additive noise:} $Y = g(X) + \epsilon$, for $\epsilon$ independent of $X$ with $\E[\epsilon] = 0$
    \item \textit{Heteroscedastic additive noise:} $Y = g(X) + h(X)\epsilon$, for $\epsilon$ independent of $X$ with $\E[\epsilon] = 0$
    \item \textit{Non-additive noise:} $Y = g(X, \epsilon)$, for $\epsilon$ independent of $X$
    \item \textit{Model-free:} the form of the underlying model is not known, only iid observations $(X_i, Y_i)_{i=1}^m$ are available
\end{myitemize}


\subtitle{Regression Function:}
\(\bar{f}(x)\) minimizes \(w \mapsto \\ \int_{\mathcal{Y}} \ell(w, y) \rho_{Y|X}(dy | x)\) and is called the \textit{regression function}. \\ \(\E[\ell(\bar{f}(X), Y)]\) is the \textit{irreducible error}.

\textbf{\textit{Square Loss}} \(\bar{f}(X) = \E[Y | X]\), \(\E[\ell(\bar{f}(X), Y)] = \Var(Y) - \Var(\E[Y | X])\) 

\textbf{\textit{Pinball Loss}}
\( \ell_{\alpha}(u) = \left( \alpha - 1_{\{u < 0\}} \right) u \text{ for } u \in \R \), \(\bar{f}(X) = q_\alpha\) and \(\E[\ell(\bar{f}(X), Y)] = \E \left[ \ell_\alpha( Y - q_\alpha(X) ) \right ])\) 

\begin{subbox}{Bias-Variance Decomposition of Square Loss}
\begin{align}
E\left[ (\hat{f}_m(X) - Y)^2 \right] = 
\underbrace{ \E \left[ \left( \hat{f}_m^{\text{avg}}(X) - \bar{f}(X) \right)^2 \right]}_{\text{Bias}^2} + \notag  \\
 \underbrace{E\left[ \left( \hat{f}_m(X) - \hat{f}_m^{\text{avg}}(X) \right)^2 \right] }_{\text{Variance}} \notag 
 + \underbrace{E\left[ \left( \bar{f}(X) - Y \right)^2 \right]}_{\text{Irreducible Error}}. \notag
\end{align}
\end{subbox}

% Insert Image Bia-Variance Tradeoff
\begin{wrapfigure}{r}{0.5\columnwidth}
    \centering
    \vspace{-20pt}
    \includegraphics[width=0.45\columnwidth]{figures/Bias_and_variance_contributing_to_total_error.pdf}
    \vspace{-30pt}
\end{wrapfigure}

\textbf{Bias}: Error from erroneous assumptions (underfitting). \\
\textbf{Variance}: Error from sensitivity to data fluctuations (overfitting). \\
\textbf{Trade-off}: Increase model complexity $\Rightarrow$ lower bias, higher var

\begin{subbox}{$R^2$ \quad (goodness of in-sample fit)}
    Let $(X_i, Y_i)_{i=1}^m$ be the training data, and $(X_i, Y_i)_{i=m+1}^{m+n}$ the test data.
    Let $\hat{f}_m: \mathcal{X} \to \mathcal{Y}$ be a prediction function trained on training data:

    \[R^2 = 1 - \frac{SSR}{SST}\]

    $\text{SSR} = \sum_{i=1}^{m} \left( Y_i - \hat{f}_m(X_i) \right)^2$, $\text{SST} = \sum_{i=1}^{m} \left( Y_i - \overline{Y}_{\text{train}} \right)^2$ for $\quad \overline{Y}_{\text{train}} = \frac{1}{m} \sum_{i=1}^{m} Y_i$. \textit{The higher the better}.
\end{subbox}

Similarly, define out-of-sample \(R^2_{\text{os}}\) using test data; it measures \textbf{prediction power}.

\begin{subbox}{Approximation Error}
    $$\inf_{f \in \mathcal{H}} \mathbb{E} \, \ell(f(X), Y) - \mathbb{E} \, \ell(\overline{f}(X), Y) \geq 0$$
\end{subbox}
Approximation error occurs if $\mathcal{H}$ is not flexible enough to approximate $\bar{f}$ well. Is decreasing if $\mathcal{H}$ is increasing. Is $0$ if $\bar{f} \in \mathcal{H}$. 

If the empirical loss min. problem $\min_{f \in \mathcal{H}} \frac{1}{m} \sum_{i=1}^{m} \ell(f(X_i), Y_i)$ has a solution, we call it $\hat{f}_m^{\mathcal{H}} \in \mathcal{H}$.

\begin{subbox}{Sampling Error}
\begin{footnotesize}
    $$\E \left[ \ell \left( \hat{f}_m^{\mathcal{H}}(X), Y \right) \middle| \hat{f}_m^{\mathcal{H}} \right] 
- \inf_{f \in \mathcal{H}} \E [\ell(f(X), Y)] = $$
$$
\int_{\mathcal{X} \times \mathcal{Y}} \ell \left( \hat{f}_m^{\mathcal{H}}(x), y \right) \rho(dx, dy) 
- \inf_{f \in \mathcal{H}} \int_{\mathcal{X} \times \mathcal{Y}} \ell(f(x), y) \rho(dx, dy) \geq 0$$
\end{footnotesize}
\end{subbox}

Results from minimizing the empirical loss instead of the expected loss. Tends to decrease if the sample size $m$ is increasing.

\begin{subbox}{Direct Optimization Error}
Let $\hat{f}_m(\cdot) = \hat{\varphi}(\cdot, (X_i, Y_i)_{i=1}^{m}, V)
$:
    $$\frac{1}{m} \sum_{i=1}^{m} \ell \left( \hat{f}_m(X_i), Y_i \right) 
- \inf_{f \in \mathcal{H}} \frac{1}{m} \sum_{i=1}^{m} \ell \left( f(X_i), Y_i \right) \geq 0
$$
\end{subbox}
For classical methods (e.g., lin. reg.), assume \(\hat{f}_m \approx \hat{f}_m^{\mathcal{H}}\). For complex ML methods (boosted trees, NNs), typically \(\hat{f}_m \not\approx \hat{f}_m^{\mathcal{H}}\).

\begin{subbox}{Generalization Error}
\begin{footnotesize}
\[
\E\left[\ell(\hat{f}_m(X), Y) \mid \hat{f}_m\right]
= \E[\ell(\bar{f}(X), Y)] \quad \textit{(irreducible error)}
\]
\[
+ \inf_{f \in \mathcal{H}} \E[\ell(f(X), Y)] - \E[\ell(\bar{f}(X), Y)] \quad \textit{(approximation error)}
\]
\[
+ \E \left[\ell\left(\hat{f}_m^{\mathcal{H}}(X), Y\right) \mid \hat{f}_m^{\mathcal{H}}\right] - \inf_{f \in \mathcal{H}} \E[\ell(f(X), Y)] \quad \textit{(sampling error)}
\]
\[
+ \E \left[\ell\left(\hat{f}_m(X), Y\right) \mid \hat{f}_m\right] - \E \left[\ell\left(\hat{f}_m^{\mathcal{H}}(X), Y\right) \mid \hat{f}_m^{\mathcal{H}}\right] \textit{(ind. opt. error)}
\]
\end{footnotesize}
\end{subbox}
For fixed \(m \in \mathbb{N}\) and increasing \(\mathcal{H}\): approx. error decreases, sampling error increases (variance of \(\hat{f}_m^{\mathcal{H}}\) grows). There's a tradeoff between them (like bias-variance). For small data, \(\mathcal{H}\) should be simple; for large data, it can be complex.

\begin{subbox}{ Training Error and Test Error}
\begin{footnotesize}
$$E_{\text{tr}} = \frac{1}{m} \sum_{i=1}^{m} \ell\left(\hat{f}_m(X_i), Y_i\right), \ \ E_{\text{te}} = \frac{1}{m} \sum_{i=m+1}^{m+n} \ell\left(\hat{f}_m(X_i), Y_i\right)$$
\end{footnotesize}
\end{subbox}

If $E_{\text{tr}} \ll E_{\text{te}}$, the data most likely was overfitted. \\
\textbf{Sample Variance:} $\sigma_{\text{te}}^2 = \frac{1}{n-1} \sum_{i=m+1}^{m+n} \left( \ell\left(\hat{f}_m(X_i), Y_i\right) - E_{\text{te}} \right)^2$

\section{Linear Regression}

Linear regression models the relationship between predictors and response:
\(
y = \beta_0 + \beta_1 X_1 + \dots + \beta_d X_d + \epsilon, \quad \epsilon \sim N(0, \sigma^2)
\)
In matrix form:
\[
y = A\beta + \epsilon
\]
where \( A \) is the design matrix, \( \beta \) the coefficient vector.

\begin{subbox}{Normal Equation}
If $A^TA$ regular (or equivalently columns of $A$ lin. indep.):
\[
\hat{\beta} = \min_{b \in \mathbb{R}^{d+1}} \|A b - y\|_2^2
 = (A^\top A)^{-1} A^\top y
\]
\end{subbox}
$\Rightarrow \hat{\beta} = (A^T A)^{-1} A^T (A \beta + \epsilon) \sim \mathcal{N}_{d+1} \left( \beta, \sigma^2 (A^T A)^{-1} \right)$

If columns of \( A \) are linearly dependent, \( A^\top A \) is singular, and the normal equation has infinitely many solutions. The \textbf{pseudoinverse} solution \(\hat{\beta} = (A^\top A)^{\dagger} A^\top y\) minimizes \( \| b \|_2 \). Here, \((A^\top A)^{\dagger} = V \Lambda^{\dagger} V^\top\), with \(\Lambda^{\dagger}\) diagonal entries \(1_{\{\lambda_j > 0\}} \lambda_j^{-1}\).


\subtitle{Singular Value Decomposition (SVD)}

Matrix \( A \in \R^{m \times l}\), rank $r \leq \min(m,l)$, can be decomposed as:
\[
A = U \Sigma V^\top 
\]

\begin{myitemize}
    \item $U \in \R^{m \times m}$ and $V\in \R^{l \times l}$ orthogonal, $\Sigma^\dagger \in \R^{l \times m}$ diagonal
    \item $\lambda_1 \geq ... \geq \lambda_r$ are the positive eigenvalues of $A^\top A$ and $\Sigma = \text{diag}(\sqrt{\lambda_1}, ..., \sqrt{\lambda_r}, 0, ...) \in \R^{m \times l}$
    \item \textbf{Pseudoinverse} $A^\dagger = V \Sigma^\dagger U^\top$; for any $y \in \mathbb{R}^m, \hat{\beta} = A^{\dagger} y \in \mathbb{R}^n$ minimizes $b \mapsto \|A b - y\|_2$ with minimal $\|\cdot\|_2$-norm.
    \item \textbf{Regularization:} Truncate small \(\sigma_i\) (set \(\sigma_i = 0\) if \(\sigma_i < c\)); then \(\hat{\beta}_c = A_c^\dagger y \sim \mathcal{N}_l \left( Q_k \beta, \sigma^2 V \Lambda_c^{-1} V^T \right)\) balances bias and variance: increasing \( c \) increases bias and decreases variance.
\end{myitemize}

\begin{subbox}{Ridge Regression}
    $$\hat{\beta}_\lambda = \min_{b \in \mathbb{R}^{d+1}} \left( \|A b - y\|_2^2 + \lambda \|b\|_2^2 \right) = (A^\top A + \lambda I_l)^{-1} A^\top Y$$
\end{subbox}

\begin{footnotesize}
$\hat{\beta}_\lambda =  \sum_{i=1}^{r} \frac{\sigma_i}{\sigma_i^2 + \lambda} v_i u_i^T y \sim \mathcal{N}_l \left( \sum_{i=1}^{r} \frac{\lambda_i}{\lambda_i + \lambda} v_i v_i^T \beta, \sigma^2 \sum_{i=1}^{r} \frac{\lambda_i}{(\lambda_i + \lambda)^2} v_i v_i^T \right)$
\end{footnotesize}
\textit{Increasing \( \lambda \) increases bias and decreases variance}. 

\begin{subbox}{LASSO Regression}
\[
\hat{\beta}_\lambda = \min_{b \in \mathbb{R}^{d+1}} \left( \|A b - y\|_2^2 + \lambda \| \beta \|_1 \right)
\]
\end{subbox}
$\Rightarrow$ No closed-form solution, LASSO-Regr. encourages \textbf{sparsity}.

\subtitle{Cross-Validation}

Technique to estimate model's predictive performance and tune hyperparameters.
\begin{subbox_noTitle}
Divide data into \( K \) folds:
\begin{itemize}
    \item Train on \( K-1 \) folds.
    \item Validate on the remaining fold.
    \item Repeat \( K \) times; average validation error.
\end{itemize}
\end{subbox_noTitle}

\subtitle{Standardized Linear Regression} 
\[
\tilde{x}_i = \frac{x_i - \bar{x}}{s_x}, \quad  \tilde{y}_i = \frac{y_i - \bar{y}}{s_y}
\]
with mean \( \bar{x}_j \) and $s_j^2 = \frac{1}{m} \sum_{i=1}^{m} \left( x_{ij} - \bar{x}_j \right)^2$. The prediction $\hat{y}$ for a new datapoint $x=(x_1,...,x_d)$ is $\hat{y} = s_y \sum_{j=1}^{d} \frac{x_j - \bar{x}_j}{s_j} \hat{\beta}_j + \bar{y}$

\section{Gradient Descent}

A function $ h : \mathbb{R}^d \to \mathbb{R}$ is said to be \textbf{\textit{convex}} if $
h(\lambda x + (1 - \lambda) y) \leq \lambda h(x) + (1 - \lambda) h(y)$, for all $ x, y \in \mathbb{R}^d \text{ and } \lambda \in (0,1)$.

\textbf{Setup}: Given a convex function \( h: \mathbb{R}^d \to \mathbb{R} \) with minimizer \( x^* \), start at \( x_0 \) (might be random) and update with gradient steps:
\[
x_{k+1} = x_k - \eta_k \nabla h(x_k)
\]

If $\|\nabla h(x_k)\|_2 \leq L$  for all  $k$, then:
\[
\min_{0 \leq k \leq K} h(x_k) - h(x^*) \leq \frac{\|x_0 - x^*\|_2^2 + L^2 \sum_{k=0}^{K} \eta_k^2}{2 \sum_{k=0}^{K} \eta_k}
\]
If \( \sum_{k=0}^\infty \eta_k^2 < \infty \) and \( \sum_{k=0}^\infty \eta_k = \infty \), then \( h(x_k) \to h(x^*) \).

For accuracy $\varepsilon$, one needs $K = \left\lceil \left( \frac{L \| x_0 - x^* \|_2}{\varepsilon} \right)^2 \right\rceil$ gradient steps (which does not depend on the dimension!).

\begin{subbox}{Stochastic Gradient Descent (SGD)}
Consider $ H : \Omega \times \mathbb{R}^d \to \mathbb{R} $, convex in $ \theta $, measurable in $ \omega $, with $ \mathbb{E}[|H(\theta)|] < \infty $ for all $ \theta \in \mathbb{R}^d $. Let $ H_i $ be independent copies of $ H $, and $ I \in \mathbb{N} $. Define the gradient estimator:

$$
g_k(\theta) = \frac{1}{I} \sum_{i=kI+1}^{(k+1)I} \nabla H_i(\theta) \approx \nabla h(\theta), \quad k \geq 0.
$$

Start with $ \theta_0 \in \mathbb{R}^d $ (random) and perform updates:
$$
\theta_{k+1} = \theta_k - \eta_k \, g_k(\theta_k), \quad k \geq 0.
$$

For $ I = 1 $: SGD; for $ I > 1 $: SGD with mini-batches.

\end{subbox}

Let $\widetilde{H}_i$, $i = 1,\dots,v$, be independent copies of $H$, independent of $H_i$ (\textit{validation set}). Monitor the \textit{empirical loss} $\frac{1}{v} \sum_{i=1}^{v} \widetilde{H}_i(\theta_k)$ for $\theta_k$, $k = 0,1,\dots$. If the validation loss stops decreasing, decrease the learning rate $\eta_k$; if it increases, stop the SGD.

\section{Logistic Regression }

\begin{subbox}{Logistic Regression $\quad \quad$ (Binary Classification)}
    Let $Y \mid X \sim \text{Ber}(p(X))$, where $X = (X_1, \dots, X_d), p(X) = \psi\left(\beta_0 + \beta_1 X_1 + \dots + \beta_d X_d\right)$ and $\psi(x) = \frac{e^x}{e^x + 1} = \frac{1}{1 + e^{-x}}$.

    The empirical loss function, derived from conditional negative log-likelihood, corresponds to the \textit{cross-entropy loss}:
    
    \begin{align*}
    \hat{b} & = \min_{b \in \mathbb{R}^{d+1}} \sum_{i=1}^{m} \left\{ -y_i \log(\psi(x_i^T b)) - (1 - y_i) \log(1 - \psi(x_i^T b)) \right\} \\ 
    & = \min_{b \in \mathbb{R}^{d+1}} \sum_{i=1}^{m} \left\{ \log(1 + e^{x_i^T b}) - y_i x_i^T b  \begingroup \color{gray} \underbrace{ + \lambda ||b||_2}_{\text{Regularization}}  \endgroup \right \}
    \end{align*}

\end{subbox}

$\Rightarrow$\textit{Convex} min. problem in $b \in \R^{d+1}$, can be solved with (S)GD. \\

Having obtained $\hat{\beta} = (\hat{\beta}_0, \dots, \hat{\beta}_d)$ from the empirical loss minimization, we predict $Y = 1$ for a new data point $X = (x_1, \dots, x_d)$ by computing:
$
\hat{p}(x) = \psi\left( \hat{\beta}_0 + \sum_{j=1}^{d} x_j \hat{\beta}_j \right).
$
Using a decision \\ threshold $c \in (0, 1)$, we predict:
$
\hat{y}(x) = \begin{cases}
1, & \text{if } \hat{p}(x) \geq c, \\
0, & \text{if } \hat{p}(x) < c.
\end{cases}
$


% Having the solution of the empirical loss minimization problem: $\hat{\beta} = (\hat{\beta}_0, . . . , \hat{\beta}_d)$, we can predict the probability $Y=1$ conditioned on a new datapoint $X = (x_1, ..., x_d)$ by $\hat{p}(x) = \psi \left( \hat{\beta}_0 + \sum_{j=1}^{d} x_j \hat{\beta}_j \right)$. We then choose a \textit{decision threshold} $c \in (0, 1)$ and predict $Y$ to be $\hat{y}(x) = 1$ if $\hat{p}(x) \geq c$ and $\hat{y}(x) = 0$ if $\hat{p}(x) < c$.

\subtitle{Performance Metrics (Diagnostics)}  
\vspace{-0.2cm} % More compact
\begin{align*}
\text{TPR (Recall)}&=\frac{TP}{P} \quad &  \text{FPR}&=\frac{FP}{N} \quad & \text{FDR}&=\frac{FP}{TP+FP}
\end{align*}
\begin{align*}
\text{Accuracy}&=\frac{TP + TN}{P+N} \quad \quad    &  \text{Precision (PPV)}&=\frac{TP}{TP+FP}            
\end{align*}
$$
\text{F1 Score} = 2 \frac{\text{TPR} \times \text{PPV}}{\text{TPR} + \text{PPV}} = \frac{2 \text{TP}}{2 \text{TP} + \text{FP} + \text{FN}} = \frac{2}{\frac{1}{\text{Precision}} + \frac{1}{\text{Recall}}}
$$

\textbf{ROC}: plots TPR vs. FPR for $c \in [0, 1]$. Random guessing produces the diagonal with $AUC = 1/2$.
\textbf{AUROC} (Area under ROC): the larger the better (1 indicates a perfect classifier).

\subtitle{Credit Analytics}

Let $P$ be a good borrower and $N$ a bad borrower.

Try to \textit{minimize} $\text{FDR} = FP/(FP + TP)$ (FPâs result in losses), Try to \textit{maximize} $\text{TPR} = TP/P$ (increases business volume). 

$\Rightarrow$ Try to obtain a flat FDR/TPR-curve, i.e., a small area under the plotted FDR/TPR-curve is desirable.


\section{Support Vector Machine (SVM)}

\textbf{Derivation (Hard-margin SVM):} 
Let $ (x_i, y_i) \in \mathbb{R}^d \times \{-1, 1\} $ for $ i = 1, \dots, m $, with linear classification, i.e., there exist $ w \in \mathbb{R}^d $ and $ b \in \mathbb{R} $ such that
$
\operatorname{sign}(\langle w, x_i \rangle + b) = y_i \text{ for all } i.
$
Each classifier $ (w, b) $ defines the decision hyperplane:
$
H(w, b) = \{ x \in \mathbb{R}^d : \langle w, x \rangle + b = 0 \}.
$
The distance from $ x_i $ to $ H(w, b) $ is:
$
d_i = \frac{|\langle w, x_i \rangle + b|}{\|w\|},
$
and the margin is the minimal distance:
$
\gamma = \min_{i} d_i.
$
We aim to maximize the margin:
$
\max_{w, b} \gamma
$
subject to:
$
y_i (\langle w, x_i \rangle + b) \geq \gamma \|w\| \quad \forall i.
$
By scaling $ w $ and $ b $ such that $ \gamma \|w\| = 1 $, the constraints simplify to:
$
y_i (\langle w, x_i \rangle + b) \geq 1,  \forall i.
$
Maximizing $ \gamma $ is equivalent to minimizing $ \|w\| $, yielding:

\begin{subbox}{Hard-margin SVM}
$$
\min_{w, b} \|w\|^2
\quad 
\text{s.t. }
y_i (\langle w, x_i \rangle + b) \geq 1 \quad  \forall i \in \{1, \dots, m\}.
$$
\end{subbox}

\textbf{Problem:} Linear separation may be infeasible or lead to overfitting. Solutions using \textit{RKHS learning}:

\begin{myitemize}
    \item Lift $x_i$ into a \textit{Hilbert space} $H_0$ via a feature map $\Phi : \mathbb{R}^d \to H_0$. The classifier becomes $w \in H_0$.
    \item Introduce \textit{slack variables} $\xi_i \geq 0$ to allow constraint violations with a penalty in the objective.
\end{myitemize}

\begin{subbox}{Soft-margin SVM (Kernelized)}
    \begin{footnotesize}
    \begin{align*}
        \hat{w}_{SVM} &=
    \min_{w,b,\xi} 
    \| w \|_2^2 + C \sum_{i=1}^{m} \xi_i  \quad 
    \text{s.t. }  \ y_i \left( \left\langle w, \Phi(x_i) \right\rangle + b \right) \geq 1 - \xi_i \\
    &= \min_{(w,b) \in H_0 \times \R} 
    \| w \|_2^2 + \lambda \sum_{i=1}^{m} \underbrace{\max \left( 0, 1-  y_i ( \left\langle w, \Phi(x_i) \right\rangle + b \right ) )}_{\text{Hinge loss}} \\
    &= \min_{f \in H} \frac{1}{m} \sum_{i=1}^{m} \ell_{\text{Hinge}}(y_i, f(x_i)) + \lambda \|f\|_H^2 \\
    & 
    {\scriptstyle
    \quad \quad \text{where } H = \{f : \R^d \to \R : f = \langle w, \Phi(\cdot) \rangle \} \text{ is a Hilbert Space}}
    \end{align*}
    \end{footnotesize}
\end{subbox}

\textbf{Support Vector Regression (SVR)}:
Find $ (w, b) \in H_0 \times \mathbb{R} $ to approximate $ y \in \mathbb{R} $. Goal: Fit data within an $ \epsilon $-tube around $ f(x) = \langle w, \Phi(x) \rangle + b $.
The optimization problem becomes: \\
$ 
\min_{(w,b,\xi)} \|w\|^2 + C \sum_{i=1}^{m} \xi_i
$
s.t. $ |y_i - \langle w, \Phi(x_i) \rangle - b| \leq \epsilon + \xi_i$ with $\xi_i \geq 0 $.
Or similarly:
$
\min_{f \in H} \lambda \|f\|_H^2 + \frac{1}{m} \sum_{i=1}^{m} \ell_{\epsilon}(y_i, f(x_i)+b) 
$
where $ \epsilon $-insensitive loss $ \ell_\epsilon(y, y') = \max(0, |y - y'| - \epsilon) $.

\newpage

\section{Kernels \& Hilbert Spaces}

\begin{subbox_noTitle}
    A function $k : \mathcal{X} \times \mathcal{X} \to \mathbb{R}$ is called a \textbf{kernel} on $\mathcal{X}$ if there exists a Hilbert space $H$ and a map $\Phi : \mathcal{X} \to H$ such that for all $x, x' \in \mathcal{X}$ we have $k(x, x') = \langle \Phi(x), \Phi(x') \rangle_H$.
\end{subbox_noTitle}

A function $k : \mathcal{X} \times \mathcal{X} \to \mathbb{R}$ is a \textbf{kernel} if and only if it is \textbf{symmetric} and \textbf{pos. semidefinite} ($ \sum_{i=1}^{n} \sum_{j=1}^{n} \alpha_i \alpha_j k(x_j, x_i) \geq 0$).

\begin{myitemize}
    \item Linear kernel: $k(x, x') = \langle x, x' \rangle$
    \item Polynomial kernel: $p \in \mathbb{N}$ and $c \in \mathbb{R}_+$, $k(x, x') = (\langle x, x' \rangle + c)^p$
    \item Gaussian kernel: for $\gamma > 0$, $k(x, x') = \exp\left(-\frac{\|x - x'\|_2^2}{\gamma^2}\right)$
    \item Compactly supported radial basis kernel: for $p > d/2$, $k(x, x') = \max\{1 - \|x - x'\|_2, 0\}^p$
    \item Radial basis function kernels: $k(x, x') = \phi(\|x - x'\|_2)$ for $\phi : \mathbb{R}_+ \to \mathbb{R}$. The factor $\gamma$, s.t. $k_\gamma(x, x') = \phi(\|x - x'\|_2/\gamma)$ is the \textit{bandwidth} (the higher, the smoother the function).
\end{myitemize} 

\textbf{Decomposition Rules:} 
$k(\mathbf{x}, \mathbf{y}) = k_1(\mathbf{x}, \mathbf{y}) + k_2(\mathbf{x}, \mathbf{y})$,
$k(\mathbf{x}, \mathbf{y}) = k_1(\mathbf{x}, \mathbf{y})k_2(\mathbf{x}, \mathbf{y})$,
$k(\mathbf{x}, \mathbf{y}) = f(\mathbf{x}) f(\mathbf{y})$,
$k(\mathbf{x}, \mathbf{y}) = c k(\mathbf{x}, \mathbf{y}) \text{ for } c > 0$,
$k(\mathbf{x}, \mathbf{y}) = k(\phi(\mathbf{x}), \phi(\mathbf{y}))$.

\subtitle{Reproducing kernel Hilbert spaces}

We call $H$ a \textbf{reproducing kernel Hilbert space (RKHS)}, if $\delta_x : H \to \mathbb{R}$ given by $\delta_x(f) := f(x)$ is continuous for all $x \in \mathcal{X}$. 

A kernel $k : \mathcal{X} \times \mathcal{X} \to \mathbb{R}$ is called a \textbf{reproducing kernel of $H$}, if $k(\cdot, x) \in H$ and $f(x) = \langle f, k(\cdot, x) \rangle$ for all $x \in \mathcal{X}, f \in H$. If such a kernel $k$ exists, we call $\Phi : \mathcal{X} \to H$ given by $\Phi(x) := k(\cdot, x)$ the canonical feature map.

\textbf{Theorem:} Every RKHS has a unique reproducing kernel, and conversely, every positive definite kernel $ k $ corresponds to a unique RKHS $ H $.

\begin{subbox}{Mercer's Theorem}
    If $k$ is a continuous, symmetric, positive definite kernel on a compact set $\mathcal{X}$, then:
    $
    k(x, x') = \sum_{i=1}^\infty \lambda_i \phi_i(x) \phi_i(x'),
    $
    where $\lambda_i \geq 0$ are eigenvalues and $\{\phi_i\}$ are orthonormal in $L^2(\mathcal{X})$. Furthermore, the RKHS $H$ associated with $k$ is:
    $
    H = \left\{ f = \sum_{i=1}^\infty a_i \phi_i : \sum_{i=1}^\infty \frac{a_i^2}{\lambda_i} < \infty \right\},
    $
    with inner product
    $
    \langle f, g \rangle_H = \sum_{i=1}^\infty \frac{a_i b_i}{\lambda_i},
    $
    for $f = \sum a_i \phi_i$, $g = \sum b_i \phi_i$.
\end{subbox}
% \textbf{Implications:} Mercer's Theorem provides the theoretical basis for expressing kernels as infinite-dimensional feature maps, allowing us to perform computations in an implicit high-dimensional space.

\begin{subbox}{Representer Theorem}
    Any minimizer $f \in H$ of the regularized empirical risk
    $
    \min_{f \in H} \lambda \|f\|_H^2 + \sum_{i=1}^{m} \ell(y_i, f(x_i))
    $
    admits a representation of the form
    $
    f^*(x) = \sum_{i=1}^{m} \alpha_i k(x_i, x)
    $
    for some coefficients $\alpha_i \in \mathbb{R}$.
\end{subbox}

\textbf{Implications:} Mercer's Theorem allows kernels to implicitly map data into high-dimensional spaces. Representer Theorem ensures solutions are finite sums over training data using kernels (\textit{kernel trick}).

\textbf{Corollary:} Let $\ell(y, y') = (y - y')^2$. Assume $x_1, \dots, x_m$ are distinct and $k$ is strictly positive definite. Then, the parameters $a = (\alpha_1, \alpha_2, \dots, \alpha_m)$ of the optimizer $f_m^* = \sum_{i=1}^{m} \alpha_i k(x_i, \cdot)$ from \textit{Representer Theorem} are given by

\[
a = (\lambda m I_m + K_m)^{-1} b,
\]

where $I_m \in \mathbb{R}^{m \times m}$ is the identity matrix, $K_m \in \mathbb{R}^{m \times m}$ is given by $K_m[i,j] = k(x_i, x_j)$, and $b = (y_1, \dots, y_m)$.

\subtitle{Numerical approaches}

\textbf{Regression:} $\hat{w} = \mathbf{\Phi}^\top \hat{\alpha}$ ( \texttt{sklearn.kernel\_ridge.KernelRidge}).
\textbf{Binary Classification:} Use SMO Algorithm. For \textit{Hinge loss} use \texttt{sklearn.svm.SVC}, for $\epsilon$-sens. loss use \texttt{sklearn.svm.SVR}. 

\vspace{0.1cm}
Analytic solution not feasible for large datasets. Solutions:

\begin{myitemize}
    \item \textbf{Feature selection}: select a subset $I \subset \{1,..., m\}, |I|= k < m$ and build an estimator of the form $\hat{f}(x) = \sum_{i \in I} \alpha_i k(x_i, x)$. E.g. NystrÃ¶m (\texttt{sklearn.kernel\_approximation.Nystroem})
    \item\textbf{Preconditioning:} Approximate $A^{-1}$ via $BB^\top \approx A^{-1}$ to solve $Ax = b$ efficiently (e.g., FALKON).
\end{myitemize}

\subtitle{Function approximation with RKHS}

Kernel $k$ is \textbf{universal}, if for every continuous function $f : X \to R$ and $\epsilon > 0$, there exists $h \in H$ such that: $\|h -f \|_{\inf} \leq \epsilon$. 

\textbf{Examples:}  Exponential kernel: $k(x, x') = \exp(\gamma \langle x, x' \rangle) \text{ for } \gamma > 0$, 
Gaussian kernel: $k(x, x') = \exp\left(-\gamma \|x - x'\|_2^2\right) \text{ for } \gamma > 0$, 
Binomial kernel: $k(x, x') = (1 - \langle x, x' \rangle)^{-\alpha} \text{ for } \alpha > 0, \ \mathcal{X} \subseteq \{x \in \mathbb{R}^d : \|x\|_2 < 1\}$.


\textbf{Kernel Rate:} For estimators $ \hat{f}_m $ in RKHS with smoothness $ s $, the estimation error decreases at rate $ m^{-\frac{s}{2s + d}} $, i.e., \\
$
\lim_{m \to \infty} \mathbb{P} \left( \| \hat{f}_m^* - \bar{f} \|_2 \geq C m^{-\frac{s}{2s + d}} \right) = 0
$

\textbf{Minimax Rate:} This rate $ m^{-\frac{s}{2s + d}} $ is the optimal rate achievable by any estimator over functions with smoothness $ s $, i.e., 
$
\lim_{m \to \infty} \inf_{\hat{T}_m} \sup_{\theta \in \Theta} \mathbb{P} \left( \| \hat{T}_m - \bar{f}_\theta \|_2 \geq c m^{-\frac{s}{2s + d}} \right) = 1.
$

$\Rightarrow$ Kernel methods achieve optimal convergence rates for estimating $ \bar{f} $, making them effective for high-dimensional nonparametric regression. However, neural networks can be seen as kernel methods where the feature map is learned from data and thus better than any a-priori fixed kernel.

\clearpage

\section{Neural Networks}

\begin{subbox}{Definition Feedforward Neural Network (FNN)}
    A \textbf{FNN} is a function $F_{\theta}: \mathbb{R}^{N_0} \rightarrow \mathbb{R}^{N_L}$ defined as:
$$
F_{\theta} = F^{(L)} \circ \rho \odot F^{(L-1)} \circ \cdots \circ \rho \odot F^{(1)}
$$
\vspace{-0.5cm}
\begin{myitemize}
    \item each $F^{(k)}: \mathbb{R}^{N_{k-1}} \rightarrow \mathbb{R}^{N_k}$ an affine function: $ F^{(k)}(x) = W^{(k)} \cdot x + b^{(k)} $, where $W^{(k)} \in \mathbb{R}^{N_k \times N_{k-1}}$ are weights and $b^{(k)} \in \mathbb{R}^{N_k}$ biases,
    \item $N_k$ the number of neurons in the $k$-th layer and $(N_0, \dots, N_L) \in \mathbb{N}^{L+1}$ is the network's architecture,
    \item $\theta = ((W^{(k)}, b^{(k)}), k = 1, \dots, L)$ are network parameters,
    \item parameter space is $\mathbb{R}^{P(N_0,\dots,N_L)} \ni \theta$, with $P := P(N_0, \dots, N_L) = \sum_{k=1}^{L} N_k N_{k-1} + N_k$,
    \item $\rho: \mathbb{R} \rightarrow \mathbb{R}$ is the non-linear activation function applied to vectors element-wise.
\end{myitemize}
\end{subbox}

\begin{myitemize}
    \item $k = 0$ is the \textit{input layer}, $k = L$ is the \textit{output layer}, $k \in \{1, \dots, L-1\}$ are the \textit{hidden layers}. $L + 1$ is the \textit{number of layers} and $L$ is the \textit{depth},
    \item $\|N\|_{\infty} = \max_{0 \leq k \leq L} N_k$ is the \textit{width} of the network.
\end{myitemize}

\begin{algorithm}
\footnotesize
\caption{Forward propagation}
\begin{algorithmic}[1]
\Require Params $\theta = \left( \left(W^{(k)}, b^{(k)}\right), k = 1, \dots, L \right)$; datapoint $(x, y)$
\Ensure Loss value $\ell(F_{\theta}(x), y)$
\State $a^{(0)} := x$
\For{$k = 1, \dots, L - 1$}
    \State $\tilde{a}^{(k)} := W^{(k)} a^{(k-1)} + b^{(k)}$
    \State $a^{(k)} := \rho(\tilde{a}^{(k)})$
\EndFor
\State $\hat{y} := W^{(L)} a^{(L-1)} + b^{(L)}$
\State \Return $\ell(\hat{y}, y)$
\end{algorithmic}
\end{algorithm}

\begin{subbox}{Mini-batch Stochastic Gradient Descent (SGD)}
    \vspace{-0.3cm}
    $$
    \theta_0 \sim \mathbb{P}_{\text{initialization}}, \quad \theta_{t+1} = \theta_t - \eta_t \nabla_{\theta} \mathcal{L}(\theta, D_{t}) \big|_{\theta = \theta_t}
    $$
    where \( D_t \) is a mini-batch of size \( B \). For \( B = 1 \), this reduces to standard SGD.
\end{subbox}

\begin{myitemize}
    \item Non-convex loss $\Rightarrow$ SGD result depends on initialization.
    \item Identical initial weights (e.g., $\theta_0 = c$) lead to identical gradients $\Rightarrow$ Initialize weights differently.
\end{myitemize}

\textbf{\textit{Xavier Initialization}:} \ \ (often used in practice)
$$
W^{(k)}_{j,l} \sim \mathcal{N}\left(0, \frac{1}{N_k}\right), \quad b^{(k)} = 0,
$$
where $N_k$ is the width of the $k$-th layer.

\begin{algorithm}
\footnotesize
\caption{Back-propagation $\quad \mathcal{O}(L)$ time \& memory}
\begin{algorithmic}[1]
\Require Params $\theta = \left( \left(W^{(k)}, b^{(k)}\right), k = 1, \dots, L \right)$; datapoint $(x, y)$
\Ensure Gradient $\nabla_{\theta} \ell(F_{\theta}(x), y)$
\State Compute $\ell(F_{\theta}(x), y)$ using forward propagation
\State $\text{\texttt{grad}} \gets \nabla_{\hat{y}} \ell(\hat{y}, y)$
\State $\nabla_{W^{(L)}} \ell(\hat{y}, y) = \text{\texttt{grad}} \cdot {a^{(L-1)}}^{T}$
\State $\nabla_{b^{(L)}} \ell(\hat{y}, y) = \text{\texttt{grad}}$
\For{$k = L - 1, \dots, 1$}
    \State $\text{\texttt{grad}} \gets \nabla_{\tilde{a}^{(k)}} \ell(\hat{y}, y) = \text{\texttt{grad}}^{T} \cdot W^{(k+1)} \cdot \text{diag} \left( \rho'(\tilde{a}^{(k)}) \right)$
    \State $\nabla_{W^{(k)}} \ell(\hat{y}, y) = \text{\texttt{grad}} \cdot {a^{(k-1)}}^{T}$
    \State $\nabla_{b^{(k)}} \ell(\hat{y}, y) = \text{\texttt{grad}}$
\EndFor
\State \Return $\left( \left( \nabla_{W^{(k)}} \ell(\hat{y}, y), \nabla_{b^{(k)}} \ell(\hat{y}, y) \right) \text{ for } k = 1, \dots, L \right)$
\end{algorithmic}
\end{algorithm}

\newpage

\subtitle{Batch Normalization} \\
\textbf{Problem:} Parameter updates cause \textit{internal covariate shift}, slowing training. \\
\textbf{Solution:} Normalize activations in each layer using batch mean and variance:
$$
a_j^{(k)}(x_i) \leftarrow \frac{a_j^{(k)}(x_i) - \mu_j^{(k)}}{\sqrt{\sigma_j^{(k)2} + \epsilon}},
$$
\begin{footnotesize}
$$
\mu_j^{(k)} = \frac{1}{n_{\text{batch}}} \sum_{i=1}^{n_{\text{batch}}} a_j^{(k)}(x_i), \quad \sigma_j^{(k)2} = \frac{1}{n_{\text{batch}}} \sum_{i=1}^{n_{\text{batch}}} \left(a_j^{(k)}(x_i) - \mu_j^{(k)}\right)^2
$$
\end{footnotesize}
$\epsilon \approx 10^{-5}$ prevents division by zero, Gradients are backpropagated through normalization.

\subtitle{Regularization:} \\
Loss functions are typically regularized, i.e. minimize objective: 

$$
\tilde{\mathcal{L}}(\theta, D) = \mathcal{L}(\theta, D) + \lambda \mathcal{R}(\theta),
$$

\textbf{Ridge:} $\mathcal{R}(\theta) = \|\theta\|_2^2$ (shrinks $\theta$, prevents overfitting) \\
\textbf{Lasso:} $\mathcal{R}(\theta) = \|\theta\|_1$ (leads to sparse $\theta$) \\
\textbf{Notes:} Only weights $W$ are regularized (not biases), and different $\lambda$ can be used per layer.

\textbf{Training Techniques as Regularizers:}

\begin{myitemize}
    \item \textbf{Early Stopping:} Stop training if no improvement on validation set for $p$ steps (patience). Limits parameter space (similar to $L^2$-regularization).
    \item \textbf{Dropout:} Set each neuron to zero with probability $p$ before each gradient step; encourages sparse, redundant represent..
    \item \textbf{Other Techniques:} Bagging, dataset augmentation, weight robustness, parameter sharing.
\end{myitemize}

\subtitle{Neural Tangent Kernel (NTK)}

\textbf{Generalization Puzzle:} Overparametrized NNs generalize well despite classical bias-variance trade-off.

\textbf{Goal:} Study the dynamics of a NN $F_\theta : \R^{N_0} \to \R$ during GD.

\begin{subbox_noTitle}
For a NN \( F_{\theta} : \mathbb{R}^{N_0} \rightarrow \mathbb{R} \), the \textbf{NTK} is defined as:
\[
K(x, x'; \theta_t) = D_{\theta_t} F_{\theta}(x) \cdot D_{\theta} F_{\theta_t}(x')^\top \in \mathbb{R}.
\]
\end{subbox_noTitle}

Loss: $\mathcal{L}(\theta_t, D) = \frac{1}{m} \sum_{i=1}^m \ell(F_{\theta_t}(x_i), y_i)$.

Gradient flow: $\frac{d\theta_t}{dt} = - D_\theta \mathcal{L}(\theta_t, D)^\top$.

Using chain rule:
$\frac{d}{dt} \mathcal{L}(\theta_t, D) = - \frac{1}{m^2} \sum_{i,j=1}^m D_{\hat{y}} \ell(F_{\theta_t}(x_i), y_i) \cdot K(x_i, x_j; \theta_t) \cdot D_{\hat{y}} \ell(F_{\theta_t}(x_j), y_j)^\top 
= - \left\| D_{\hat{y}} \ell(F_{\theta_t}(\cdot), y) \right\|_{K(\cdot, \cdot, \theta_t), D}^2$


\subtitle{Convergence Results} [Jacot et. al, 2018]

\textbf{Theorem (NTK at Initialization):} For shallow NN $(N_0,N_1,1)$ with $\theta_0 \sim \mathcal{N}(0,I)$, as $N_1 \to \infty$:
$F_{\theta_0} \to \text{GP}(0,C)$ where
$C(x,x') = \mathbb{E}_{f \sim \text{GP}(0,\Sigma)}[\rho(f(x))\rho(f(x'))] + \beta^2$
with $\Sigma(x,x') = \frac{x^\top x'}{N_0} + \beta^2$

\textbf{Theorem (NTK During Training):} For any $T>0$ with bounded $\int_0^T \frac{1}{m}\sum_i \|D_{\hat{y}}\ell(F_{\theta_t}(x_i),y_i)\|_2^2dt$:
$\sup_{t\in[0,T]}\|K(x,x';\theta_t) - \tilde{K}(x,x')\| \xrightarrow{P} 0$ as width $\to \infty$.   

\subtitle{Minimum-Norm Solution}

For squared loss $\ell(\hat{y},y) = (\hat{y}-y)^2$, as $t \to \infty$:
$F_{\theta_\infty}(x) = F_{\theta_0}(x) + \sum_{i=1}^m \sum_{j=1}^m \tilde{K}^{(L)}(x,x_i)\bar{K}_{i,j}^{-1}(y_j - F_{\theta_0}(x_j))$
where $\bar{K}_{i,j} := \tilde{K}^{(L)}(x_i,x_j)$ (limiting NTK evaluated on dataset), First term is initial network output
and Second term is correction towards minimum-norm interpolator in RKHS.

\textbf{Key insight:} Solution converges to minimum-norm interpolator in NTK RKHS:
$\arg\min_{f \in \mathcal{H}, f(x_i)=y_i} \|f\|_{\mathcal{H}}$.

\subtitle{Generalization in Overparametrized Regime}

\textbf{Linear Regression Setting:}
Model: $Y = X^\top\beta + \epsilon$, $\epsilon \sim \mathcal{N}(0,\sigma^2)$, where $X=(X_1,...,X_p) \in \R^p$ with covariance $\Sigma$. $\rightarrow \hat{\beta} = (A^\top A)^\dagger A^\top Y$.

\textbf{Bias-Variance Decomposition} (conditioned on $A$): \vspace{-10pt}
$$\mathbb{E}[(\hat{f}_D(X) - f^*(X))^2|A] = \underbrace{\beta^\top\Pi\Sigma\Pi\beta}_{B_A=\text{bias}^2} + \underbrace{\frac{\sigma^2}{m}\text{Tr}(\hat{\Sigma}^\dagger\Sigma)}_{V_A=\text{variance}}$$
where $\hat{\Sigma} = \frac{1}{m}A^\top A$, $\Pi = I_p - \hat{\Sigma}^\dagger\hat{\Sigma}$

\textbf{Theorem (Asymptotic Variance):}
For $p,m\to\infty$ with $p/m\to\gamma$:
$V_A \to \sigma^2\frac{1-\max{1-\gamma,0}}{|1-\gamma|}$. \textit{[Hastie et al., 2022]}

\textbf{Double Descent Phenomenon:} $\gamma$ quantifies model complexity ($\gamma \gg 1 \Rightarrow p \gg m \Rightarrow $ Complex model). Variance increase for $\gamma \in (0,1)$ (classical bias-variance trade-off), but decreases for $\gamma > 1$ (overparametrized).


\begin{wrapfigure}{r}{0.5\columnwidth}
    \centering
    \vspace{-20pt}
    \includegraphics[width=0.45\columnwidth]{figures/double_descent_curve.jpg}
    \vspace{-30pt}
\end{wrapfigure}

$\Rightarrow$ Variance decreases as soon as model complexity passes the interpolation threshold. The test error can decrease even after the train error has reached zero.

\newpage

\section{Convolutional Neural Networks}

Three components: \textit{convolution-}, \textit{detector-} and \textit{pooling layer}.

\textbf{Convolutional layer:} Let $I \in \mathbb{R}^{n_1 \times n_2}$, $K \in \mathbb{R}^{m_1 \times m_2}$, $m_1 \leq n_1$, $m_2 \leq n_2$, then their \textbf{cross-correlation} ($I \circledast K$) is:
$$
(I \circledast K)_{i,j} := \sum_{m=1}^{m_1} \sum_{n=1}^{m_2} I_{m+i-1, n+j-1} K_{m, n}.
$$

\begin{myitemize}
    \item \textbf{Kernels:} Square $m \times m$ matrices ($m$ hyperparameter); convolution layer uses $M > 1$ kernels in parallel to produce multiple feature maps.
    \item \textbf{Padding ($p$):} 0's added around input to control output size.
    \item \textbf{Stride ($s$):} steps kernel moves over input matrix.
\end{myitemize}

Output size formula:$\left( \frac{n_1 + 2p - m}{s} + 1, \frac{n_2 + 2p - m}{s} + 1, M \right)$ \\

\textbf{Detector layer:}
Applies non-linear activation $\rho$ component-wise ($I_{i,j,k} \mapsto \rho(I_{i,j,k})$); output shape same as input; no trainable parameters.

\begin{wrapfigure}{r}{0.25\columnwidth}
    \centering
    \vspace{-25pt}
    \includegraphics[width=0.27\columnwidth]{figures/Visual-representation-of-pooling-operations-a-max-pooling-b-average-pooling}
    \vspace{-20pt}
\end{wrapfigure}

\textbf{Pooling Layer:} Fixed filter (e.g., $m=2$, $s=2$, $p=0$); operates per channel; no trainable parameters.
\\

Convolution $\equiv$ Linear transformation via sparse, parameter-shared weights (doubly block-circulant matrix).

\begin{myitemize}
    \item \textbf{Parameter sharing:} Same kernel used at every position $\Rightarrow$ shared learning, translation equivariance.
    \item \textbf{Sparsity:} Small kernels reduce parameters $\Rightarrow$ computational efficiency; layers extract local (edges) to global (digits) features.
\end{myitemize}

\section{Recurrent Neural Networks}

\begin{wrapfigure}{r}{0.5\columnwidth}
    \centering
    \vspace{-25pt}
    \includegraphics[width=0.5\columnwidth]{figures/RNN_cell.png}
    \vspace{-20pt}
\end{wrapfigure}

Exploit data priors of sequential data, $(x^{\langle 1 \rangle}, x^{\langle 2 \rangle}, \dots, x^{\langle T \rangle})$.

$$
\begin{cases}
a^{\langle t \rangle} = W \cdot h^{\langle t-1 \rangle} + U \cdot x^{\langle t \rangle} + b \\
h^{\langle t \rangle} = \rho(a^{\langle t \rangle}) \\
y^{\langle t \rangle} = V \cdot h^{\langle t \rangle} + c
\end{cases}
$$

Hidden states of RNN form a dynamical system with stationary
transition functions: $h^{\langle t \rangle} = f_{\theta}\left(h^{\langle t-1 \rangle}, x^{\langle t \rangle}\right)$. The parameter $\theta = (W, U, b)$ are used at each time step $t$ (\textbf{parameter sharing}). Hidden state $h^{\langle t \rangle}$ summarizes statistics of past sequence. \\

\textbf{Back-Propagation Through Time (BPTT):} Consider Loss $\mathcal{L} = \sum_{t=1}^{T} \mathcal{L}^{\langle t \rangle} = \sum_{t=1}^{T} \ell(\hat{y}^{\langle t \rangle}, y^{\langle t \rangle}), \quad t = 1, \dots, T$

Compute recursively the gradients of $\mathcal{L}$:
$
D_{\hat{y}^{\langle t \rangle}} \mathcal{L} = D_{\hat{y}^{\langle t \rangle}} \ell(\hat{y}^{\langle t \rangle}, y^{\langle t \rangle})
$ and
$
D_{h^{\langle t \rangle}} \mathcal{L} = D_{\hat{y}^{\langle t \rangle}} \mathcal{L} \cdot V + D_{h^{\langle t+1 \rangle}} \mathcal{L} \cdot \operatorname{diag}\left( \rho'\left(a^{\langle t+1 \rangle}\right) \right) \cdot W
$

\textbf{Vanishing/Exploding Gradient Problem:} Gradients increase/decrease exponentially over time steps:
$$
D_{\theta} \mathcal{L}^{\langle t \rangle} = \sum_{k=1}^{t} \left( \prod_{i=k+1}^{t} D_{h^{\langle i \rangle}} h^{\langle i+1 \rangle} \right) D_{h^{\langle k \rangle}} \mathcal{L}^{\langle t \rangle} \cdot D_{\theta} h^{\langle k \rangle}
$$
If $ \| D_{h^{\langle i \rangle}} h^{\langle i+1 \rangle} \| > 1 $, gradients \textit{explode}; if $ < 1 $, gradients \textit{vanish}.

\textbf{Solution to exploding gradient problem:} \textit{gradient clipping}; at each gradient step, if the gradient is larger than a threshold $K$, 'clip' it to $K$:

$$
D_\theta \mathcal{L} \leftarrow 
\begin{cases} 
D_\theta \mathcal{L} & \text{if } \|D_\theta \mathcal{L}\| < K \\ 
K \frac{D_\theta \mathcal{L}}{\|D_\theta \mathcal{L}\|} & \text{if } \|D_\theta \mathcal{L}\| \geq K 
\end{cases}
$$

\textbf{Solutions to vanishing gradient:}

\textbf{(1) GRU:} Two gates control information flow (by learning optimal gate parameters, model learns how to accumulate/forget past info dynamically):
$$\begin{cases}
\Gamma_r = \rho(W_r h^{\langle t-1 \rangle} + U_r x^{\langle t \rangle} + b_r)  \quad \quad \text{(reset)} \\
\Gamma_u = \rho(W_u h^{\langle t-1 \rangle} + U_u x^{\langle t \rangle} + b_u) \quad \quad \text{(update)} \\
h^{\langle t \rangle} = (1-\Gamma_u) \odot h^{\langle t-1 \rangle} + \Gamma_u \odot \tanh(W[\Gamma_r \odot h^{\langle t-1 \rangle}, x^{\langle t \rangle}])
\end{cases}$$

\textbf{(2) LSTM:} Memory cell $c^{\langle t \rangle}$ separates information flow:
$$\begin{cases}
\Gamma_f, \Gamma_i, \Gamma_o = \text{forget/input/output gates} \\
c^{\langle t \rangle} = \Gamma_f \odot c^{\langle t-1 \rangle} + \Gamma_i \odot \tilde{c}^{\langle t \rangle} & \text{(memory)} \\
h^{\langle t \rangle} = \Gamma_o \odot \tanh(c^{\langle t \rangle}) & \text{(output)}
\end{cases}$$


\section{Classification \& Regression Trees}

\textbf{Key Idea:} Transform input x into output y via sequence of simple binary decisions. \ \ \textbf{Advantages:}

\begin{itemize}
    \item Easy to interpret (medicine, insurance)
    \item Powerful with ensemble methods
    \item Can approximate any continuous function
\end{itemize}

\begin{subbox}{Binary Tree Definition}
    Triplet $(\mathcal{T}, \mathcal{P}, \mathcal{V})$ where:
    
\begin{myitemize}
    \item $\mathcal{T} \subseteq \bigcup_{l \in \mathbb{N}_0} \{0, 1\}^l$: nodes ($t^{\text{flip}}, t^{\text{cut}} \in \mathcal{T}$ if $t \in \mathcal{T} \setminus \{()\}$)
    \item $\mathcal{P} = \{P_t \subseteq \mathcal{X}\}$: partitions ($P_t \cup P_{t^{\text{flip}}} = P_{t^{\text{cut}}}$)
    \item $\mathcal{V} = \{y_t \in \mathcal{Y}\}$: values
\end{myitemize}
Tree function: $f(x) = \sum_{t \in \mathcal{T}} y_t \mathds{1}_{P_t}(x)$
\end{subbox} 

\begin{algorithm}[ht!]
\footnotesize
\caption{Basic structure of tree growing algorithm}
\begin{algorithmic}[1]
\State Initialize $\mathcal{T} = \{()\}$ and $P_{()} = \mathcal{X}$.
\While{Stopping criterion (*) is not reached}
    \State Choose a node $t \in \mathcal{T}$ ($t \in \{0, 1\}^l$), a variable $i \in \{1, \ldots, d\}$, and a set $C_i \subseteq \mathcal{X}_i$ according to criterion (**).
    \State Set $\mathcal{T} \leftarrow \mathcal{T} \cup \{t0, t1\}$, where $t0 = (t_1, t_2, \ldots, t_l, 0)$ and $t1 = (t_1, t_2, \ldots, t_l, 1)$.
    \State Set $P_{t0} = P_t \cap \{x \in \mathcal{X} : x_i \in C_i\}$, $P_{t1} = P_t \cap \{x \in \mathcal{X} : x_i \not\in C_i\}$.
\EndWhile
\State Compute the values $\mathcal{V} = \{y_t : t \in \mathcal{T}\}$; hereby, for all $t \in \mathcal{T}$, $y_t$ is calculated solely using the training data $\{y^i : i \in \{1, \ldots, m\}, x^i \in P_t\}$ according to criterion (***).
\State \Return $(\mathcal{T}, \mathcal{P}, \mathcal{V})$.
\end{algorithmic}
\end{algorithm}

\newpage
\subtitle{(***) How to assign values to leaves?}

\textbf{Node Statistics:} $p(t) = \frac{|\{i: x^i \in P_t\}|}{m}$, $p(y|t) = \frac{|\{i: x^i \in P_t, y^i = y\}|}{|\{i: x^i \in P_t\}|}$ Here, $p(t)$ is the estimated probability of an input $x$ belonging to the set $P_t$, and $p(y | t)$ the likelihood of $y$ being the output given that an input $x$ belongs to the set $P_t$. 

\textbf{Regression ($\mathcal{Y} \in \R$):} $y_t = \frac{1}{|\{i: x^i \in P_t\}|}\sum_{i: x^i \in P_t} y^i$ (emp. mean)

\textbf{Classification ($\mathcal{Y} = \{1,...,K \}$):} $y_t = \arg\max_{c \in \mathcal{Y}} |\{i: x^i \in P_t, y^i = c\}|$ (majority vote)

\textbf{General:} $y_t \in \arg\min_{y \in \mathcal{Y}} \sum_{i=1}^m L(y,y^i) \mathds{1}_{x^i \in P_t}$ (empirical mean for $L(y, y')=(y-y')^2$, majority vote for $L(y, y')=\mathds{1}_{y \neq y'}$.


\subtitle{(**) How to grow a tree/split nodes?}

\textbf{Tree Impurity:} For tree structure $(\mathcal{T}, \mathcal{P})$: 
$I(t) \in [0,\infty)$ is impurity of node $t \in \mathcal{T}$, $\bar{I}(\mathcal{T}) = \sum_{t \in \mathcal{T}} p(t)I(t)$ is impurity of $\mathcal{T}$. \textit{Lower impurity is better}. $\Rightarrow$ The impurity determines the loss function which the overall tree function wants to minimize.

\textbf{Optimal Split:} $(t,i,C_i) \in \arg\min_{(t',i',C_{i'})} \bar{I}(\mathcal{T}^{(t',i',C_{i'})})$


\textbf{Classification:} $I(t) = \phi(p(1|t),\ldots,p(K|t)) \Rightarrow \bar{I}(T) \equiv \text{cross-entropy}$
    $$\begin{cases}
    \text{Gini: } \phi(p) = \frac{1}{2}\sum_{i=1}^K p_i(1-p_i) \\
    \text{Entropy: } \phi(p) = -\sum_{i=1}^K p_i\log(p_i)
    \end{cases}$$

\textbf{Regression:} $I(t) = \text{Var}(\{y^i: x^i \in P_t\}) \Rightarrow \bar{I}(T) \equiv \text{square loss}$

\subtitle{(*) Stopping criterion/regularization}
\textbf{Problem:} Without stopping, tree splits until zero loss (overfitting). \textit{Solution:} Add penalty term $\alpha$ for complexity, i.e., Penalizes number of leaves, locally defined for each node.

$I_\alpha(t) = I(t) + \frac{\alpha}{p(t)} \quad \quad \quad 
\bar{I}_\alpha(\mathcal{T}) = \bar{I}(\mathcal{T}) + \alpha|\mathcal{T}| = \sum_{t \in \mathcal{T}}p(t)I_\alpha(t)$


\textbf{Split Criterion:} Split node $t \in \mathcal{T}$ if it decreases $\bar{I}_\alpha$, i.e., if:
\begin{scriptsize}
$$
\min_{(i, C_{i})} \bar{I}_\alpha\left(\mathcal{T}^{(t, i, C_{i})}\right) < \bar{I}_\alpha(\mathcal{T}) \Leftrightarrow 
\min_{i,C_i}(p(t0)I(t0) + p(t1)I(t1)) < p(t)I(t) - \alpha
$$
\end{scriptsize}

\textbf{Choice of $\alpha$:} Grow large tree (small $\alpha$), then prune (increase $\alpha$). Pruning exploits: $\alpha$ only affects \textit{number} of splits, not their \textit{structure}. Cross-validation efficient through pruning sequence.

\section{Bagging \& Random Forests}

\textbf{Key Idea:} Combine multiple predictors to reduce variance.\\
\textbf{Regression} (Average): $\hat{f}(x) = \frac{1}{K} \sum_{k=1}^{K} \hat{f}^{(k)}(x)$ \\
\textbf{Classification} (Maj. Voting): $\hat{f}(x) \in \arg \max_{y \in \mathcal{Y}} \sum_{k=1}^{K} 1_{\hat{f}^{(k)}(x) = y}$

\textbf{Key Assumption:} Estimators $\hat{f}^{(1)},...,\hat{f}^{(k)}$ not strongly positively correlated!

\subtitle{Bootstrapping} 

Resampling technique which generates additional artificial training data sets.

\begin{algorithm}[H]
\scriptsize
\caption{Bootstrap algorithm}
\begin{algorithmic}[1]
\State Set $K \in \mathbb{N}$
\For{$k = 1, \dots, K$}
    \State \textcolor{gray}{\textbf{Non-Parametric:} Draw samples $\tilde{Z}_1^{(k)},\ldots,\tilde{Z}_m^{(k)}$ i.i.d. with repl.}
    \State \textcolor{gray}{\textbf{Parametric:} Fit distribution $\lambda(\varepsilon)$ to data, sample from it}
    \State $\hat{\theta}^{(k)} := g(\tilde{Z}_1^{(k)}, \dots, \tilde{Z}_m^{(k)})$
\EndFor
\State Return $(\hat{\theta}^{(1)}, \dots, \hat{\theta}^{(K)})$ \textit{(bootstrap-distr. $\kappa^{(m, K)} = \frac{1}{K} \sum_{k=1}^K \delta_{\hat{\theta}^{(k)}}$)}
\end{algorithmic}
\end{algorithm}

\subtitle{Bagging} (\textbf{B}ootstrapping with \textbf{Agg}regat\textbf{ing}) \\
Aggregating is a technique to combine various models/estimators into a single (more powerful) estimator.

\textbf{Subbagging:} Use heuristic $\tilde{m} = \lceil m / 2 \rceil$.

\begin{algorithm}[H]
\footnotesize
\caption{Bagging Algorithm (non-parametric)}
\begin{algorithmic}[1]
\For{$k = 1,\ldots,K$}
    \State Draw bootstrap sample $\tilde{Z}_1^{(k)},\ldots,\tilde{Z}_m^{(k)}$ with replacement
    \State Take $\hat{f}^{(k)} := \arg \min_{f \in \mathcal{F}} \frac{1}{m} \sum_{i=1}^m \ell(\tilde{Y}_i^{(k)}, \tilde{X}_i^{(k)}) + \lambda R(f)$
\EndFor
\State \Return Aggregated $\hat{f}^{agg}$ (Averaging, majority vote, or similar)
\end{algorithmic}
\end{algorithm}

\subtitle{Random Forests}

\textbf{Idea:} Modify Bagging for more variability in $\hat{f}^{(k)}$, i.e., Randomize tree generation.

\begin{algorithm}[H]
\footnotesize
\caption{Random Forest Algorithm (non-parametric)}
\begin{algorithmic}[1]
\For{$k = 1,\ldots,K$}
\State Draw bootstrap sample $\tilde{Z}_1^{(k)},\ldots,\tilde{Z}_m^{(k)}$ with replacement
\State Take $\hat{f}^{(k)}$ via randomized tree growing:
\State \quad Sample feature subset $U \subset \{1,\ldots,d\}$
\State \quad Choose best split among features in $U$
\EndFor
\State \Return Aggregated $\hat{f}^{agg}$ (Averaging, majority vote)
\end{algorithmic}
\end{algorithm}

\textbf{Benefits:}  Internal randomness reduces overfitting, increases model diversity and generalization.
\newpage


\section{Gradient boosted trees}

\textbf{Key Problem:} Given binary classification data $D = \{(x_i, y_i) \in \mathcal{X} \times \{-1,+1\}\}_{i=1}^m$ and weak learners (slightly better than random), how to build a strong classifier?

\textbf{Main Idea:} Sequential training of weak learners on reweighted data, focusing on previously misclassified points. Then aggregate them into a strong classifier using error-weighted outputs.

\subtitle{AdaBoost Algorithm}

\begin{algorithm}[H]
\footnotesize
\caption{AdaBoost}
\begin{algorithmic}[1]
\State Initialize \textit{data weights} $w_i^{(1)} = 1$ for $i=1,\ldots,m$
\For{$k = 1,\ldots,K$}
    \State Fit $C_k = \arg\min_C \sum_{i=1}^m w_i^{(k)}\mathds{1}_{\{C(x_i) \neq y_i\}}$
    \State Compute weighted error rate $\text{err}_k := \frac{\sum_{i=1}^m w_i^{(k)}\mathbb{1}\{C_k(x_i) \neq y_i\}}{\sum_{i=1}^m w_i^{(k)}}$
    \State Compute classifier's weight $\alpha_k := \log(\frac{1-\text{err}_k}{\text{err}_k})$
    \State Update data weights $w_i^{(k+1)} := w_i^{(k)}\exp(\alpha_k\mathds{1}_{\{C_k(x_i) \neq y_i\}})$
\EndFor
\State \Return $C(x) = \text{sign}(\sum_{k=1}^K \alpha_k C_k(x))$
\end{algorithmic}
\end{algorithm}

\textbf{Properties:} Always assume $\text{err}_k < \frac{1}{2}$ (else flip predictions). Weight updates increase misclassified points:
\begin{scriptsize}
$$w_i^{(k+1)} = \begin{cases}
w_i^{(k)} & \text{if } C_k(x_i) = y_i\\
w_i^{(k)}(\frac{1-\text{err}_k}{\text{err}_k}) & \text{if } C_k(x_i) \neq y_i
\end{cases}$$
\end{scriptsize}
$\Rightarrow$ Next weak classifier focuses more on misclassified samples.

\subtitle{Stage-wise Adaptive Modeling}
\textbf{Additive Expansion:} Build model sequentially: $f(x) = \sum_{k=1}^K \beta_k g_{\theta_k}(x)$
where $g_{\theta_k}$ are weak learners with parameters $\theta_k$

\textbf{AdaBoost as Special Case:}
Uses exponential loss: $\ell(f(x),y) = \exp(-yf(x))$. Optimal prediction: $f^*(x) = \frac{1}{2}\log(\frac{P(Y=1|X)}{P(Y=-1|X)})$. Weak learners: $g_{\theta_k}(x) = C_k(x)$ with weights $\beta_k = \alpha_k$.

\subtitle{Gradient Boosted Trees}

\textbf{Idea:} Stage-wise adapt. modelling using trees as weak learners. \\
\textbf{Model:}$f_K(x) = \sum_{k=1}^K g_k(x)$ where $g_k=(T_k, P_k, V_k)$ small trees

\begin{footnotesize}
\begin{algorithm}[H]
\footnotesize
\caption{Gradient Tree Boosting Algorithm}
\begin{algorithmic}[1]
\State Initialize weights $f_0(x) = 0$.
\For{$k = 1, \ldots, K$}
    \State Compute the \textit{pseudo-residuals}: 
    \(
    r_{k,i} = - \left[ \frac{\partial \ell(\hat{y}, y_i)}{\partial \hat{y}} \right]_{\hat{y} = f_{k-1}(x_i)}
    \) 
    \State Fit a square loss regression tree $g_k = (T_k, P_k, V_k)$ to $(r_{k,i})_{i=1}^m$.
    \State Set $f_k(x) = f_{k-1}(x) + g_k(x)$.
\EndFor
\State \textbf{Return} $f_K$.
\end{algorithmic}
\end{algorithm}
\end{footnotesize}


\textbf{Pseudo-residual for Square loss:} $r_{k,i} = y_i - f_{k-1}(x_i)$.

\textbf{Regularization:} Restrict Max. number of leaves $J$ (typically 4-8), Learning rate $\eta$ in update $f_k = f_{k-1} + \eta g_k$, Subsampling fraction $\alpha$ of data.


\section{Dim. Reduction \& Autoencoders}

\textbf{Goal:} Represent high-dimensional data in low-dimensional space (\textit{latent space}) by preserving as much information as possible.

\subtitle{Principal Component Analysis (PCA)}

\textbf{Main idea:} Project data onto low-dimensional subspace while preserving max. amount of variance.

Let $A$ be the standardized data (centered and unit variance)

\textbf{Optimization Problem:} Find orthonormal basis $\mathbf{v}_1, \dots, \mathbf{v}_d$ that maximize variance:
$
\mathbf{v}_1 = {\arg\max}_{\|\mathbf{w}\|_2=1} \|A\mathbf{w}\|_2^2
$

\textbf{Solution:} Eigenvectors of $A^T A$ (covariance matrix) corresponding to top eigenvalues $\sigma_1^2 \geq \sigma_2^2 \geq \dots$.

\textbf{Dimensionality Reduction:} Represent data using $k$ top principal components, i.e., $\mathbf{x}_i \mapsto (\mathbf{x}_i^T \mathbf{v}_1, \dots, \mathbf{x}_i^T \mathbf{v}_k).
$

\textbf{SVD Solution:} $A = U\Sigma V^T = \sum_{i=1}^r \sigma_i \mathbf{u}_i \mathbf{v}_i^T$. Where $\mathbf{v}_i$: principal components (eigenvectors of $A^TA$), $\sigma_i$: singular values. \\
$\Rightarrow$ Low-dim representation: $A_k = \sum_{i=1}^k \sigma_i \mathbf{u}_i \mathbf{v}_i^T = U_k\Sigma_kV_k^T.$

\textbf{Explained Variance:} $A_k$ preserves
$
\frac{\sum_{i=1}^k \sigma_i^2}{\sum_{i=1}^d \sigma_i^2}
$
of total variance.

\subtitle{Kernel PCA}

\textbf{Key Idea:} Extend PCA to non-linear transformations. \\ For kernel $k(x,x')=\langle \Phi(x), \Phi(x') \rangle_H$:
(1) Compute $K_{ij} = k(x_i,x_j)$, (2) Center: $\tilde{K} = K - U_mK - KU_m + U_mKU_m$, (3) Solve $\tilde{K}\alpha = \lambda\alpha$, (4) Project: $\langle \mathbf{v}_k, \Phi(x) \rangle = \sum_{i=1}^m \alpha_i^{(k)} k(x_i,x)$.

\subtitle{Auto-encoders}

\textbf{Key Idea:} Learn latent representation automatically using NNs.
\begin{wrapfigure}{r}{0.45\columnwidth}
    \centering
    \vspace{-15pt}
    \includegraphics[width=0.45\columnwidth]{figures/Autoencoder.png}
    \vspace{-40pt}
\end{wrapfigure}
\begin{itemize}
    \item \textcolor{green}{Encoder} $F_\theta : \mathbb{R}^{N_0} \to \mathbb{R}^K$
    \item \textcolor{red}{Decoder} $G_{\tilde{\theta}} : \mathbb{R}^K \to \mathbb{R}^{N_0}$
    \item \textbf{Goal:} Minimize \textit{dissimiliarity} function $\ell: \mathbb{R}^{N_0} \times \mathbb{R}^{N_0} \mapsto \R_+$: 
\end{itemize}
$$
\arg \min_{(\theta, \tilde{\theta})} \frac{1}{m} \sum_{i=1}^m \ell(G_{\tilde{\theta}}(F_\theta(x_i)), x_i)
$$

\begin{itemize}
    \item \textbf{Sparse Auto-encoders (SAEs):} Penalty on latent activations. Encourage sparsity: $\mathcal{R}(F_{\theta}(x)) = \lambda |F_{\theta}(x)|_1$.
    \item \textbf{Denoising Auto-encoders (DAEs):} Train on corrupted inputs $\tilde{x} \sim C(\cdot | x)$. Minimize $\ell(x, G_{\hat{\theta}}(F_{\theta}(\tilde{x})))$. 
    \item \textbf{Contractive Auto-encoders (CAEs):} Penalty on input gradient: $\lambda |\nabla_x F_{\theta}(x)|_F^2$. Encourage robust latent representations (i.e., representations that locally do not change much).
\end{itemize}

\textit{\textbf{Example: Categorical Feature Embedding}}

Compress high-dimensional one-hot encoded categorical features (common for insurance datasets) using autoencoders into low-dimensional embeddings; then use these latent representations in the model.


\section{GNNs \& Transformers}

\textbf{Key Idea:} Combine flexibility of NNs with relational structure learning. GNNs leverage \textit{inductive biases} to learn from graph-structured data, capturing relationships between entities.

\subtitle{Graph Neural Networks}

\textbf{Graph Data:} $G=(V,E)$ with Node data: $x_v \in \mathbb{R}^{d_V}$ for $v \in V$; Edge data: $x_e \in \mathbb{R}^{d_E}$ for $e \in E$; and Global data: $u \in \mathbb{R}^{d_U}$.

\textbf{Aggregation functions:} used to aggregate information across all edges
connected to a given node (with varying \#edges).

\begin{subbox}{Graph Net Layer}
    Transforms graphical input data into graphical output data of the same shape.
\begin{enumerate}
    \item \textbf{Edge transform:} $x_e' = \phi^E(x_e, x_v, x_w, u)$ for $e=(v,w)$. \\
    Edge $\rightarrow$ node agg.: $a_w = \rho^{E\to V}(\{x_e': e=(v,w)\})$. \\
    Edge $\rightarrow$ global agg.: $a_E = \rho^{E\to U}(\{x_e': e \in E\})$
    \item \textbf{Node transform:} $x_v' = \phi^V(a_v, x_v, u)$ \\
    Node $\rightarrow$ global agg.: $a_V = \rho^{V\to U}(\{x_v': v \in V\})$
    \item \textbf{Global transform:} $u' = \phi^U(a_E, a_V, u)$
\end{enumerate}
\end{subbox}

$\Rightarrow$ Reason for efficiency of GNNs is that function $\phi^V$ (respectively its parameters) is \textit{shared among all nodes}, and similarly $\phi^E$ is \textit{shared among all edges}.

\subtitle{Self-Attention Layer}

Feedforward NNs $f_K, f_Q, f_V : \mathbb{R}^d \to \mathbb{R}^d$ (\emph{\textbf{K}eys, \textbf{Q}ueries, \textbf{V}alues})
\begin{scriptsize}
\begin{itemize}
    \item \textbf{Edge transformation:}
    \(
    \phi^E(x_{(v,w)}, x_v, x_w) = \left( A_{(v,w)}, \nu_{(v,w)} \right) := \\ \left( \frac{\langle f_Q(x_w), f_K(x_v) \rangle}{\sqrt{d}}, f_V(x_v) \right) \in \mathbb{R}^{d+1}.
    \)
    \item \textbf{Softmax aggr. ($\rho^{E \mapsto V}$):}
    \(
    a_w = \sum_{e \in E_w} \frac{\exp(A_e)}{\sum_{e' \in E_w} \exp(A_{e'})} \nu_e \in \mathbb{R}^d.
    \)
    \item \textbf{Node transformation:}
    \(
    x'_v = \phi^V(a_v, x_v) = f_O(a_v) \in \mathbb{R}^d.
    \)
\end{itemize}
\end{scriptsize}


\subtitle{Transformers}

\textbf{Key Idea:} Stack self-attention layers for seq. data (esp. text).

\begin{itemize}
    \item Input tokens $w_1,...,w_n$ â embeddings in $\mathbb{R}^d$
    \item Learning objective: predict probability distribution over next token $w_{n+1}$ given $w_1,...,w_n$ (conditional prob. forecasting).
    \item Graph: Full $(V\times V)$ or Causal $\{(i,j): i<j\}$ (i.e., masked attention, so  token at position $j$ can only "pay attention to" tokens at positions $i< $j).
    \item Layer: Multi-head attention + Norm + Residuals
    \item Loss: $\ell(p,y)=\sum_{i=1}^K y_i\log(p_i)$
\end{itemize}

\textbf{Training:}
\textbf{(1) Pretrain:} Large-scale unsupervised (e.g., next token); \textbf{(2) Finetune:} Task-specific via full / frozen / PEFT / LoRA $(W \to W + AB, A\in\mathbb{R}^{d\times r}, B\in\mathbb{R}^{r\times k})$ / Soft-prompting.

\end{small}
\end{document}
